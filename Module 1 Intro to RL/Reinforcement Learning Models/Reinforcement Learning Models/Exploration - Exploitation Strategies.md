# Summary

- **Optimistic Initial Values**
- **Epsilon-Greedy**
    - Greedy action selection: $A_t = \argmax_a Q_t(a)$

    - $\epsilon$-Greedy with random action selection:$\begin{cases}
       A_t = \argmax_a Q_t(a) &\text{if } \epsilon >= \text {n} \\
       A_t = \text {random action} &\text{if } \epsilon < n
    \end{cases}$
        - Where *n* represents a random number
- **Upper Confidence Bound**
    - Upper-Confidence Bound action selection uses uncertainty in the action-value estimates for balancing exploration and exploitation
    - Since there is inherent uncertainty in the accuracy of the action-value estimates when we use a sampled set of rewards thus UCB uses uncertainty in the estimates to drive exploration
- **Thompson Sampling (Bayesian)**
- Gittins Index
    - Works well for Bandit problems but doesn’t seem to generalize to other RL problems

# **Greedy in the limit (GLIE) schemes**

- There are several GLIE schemes:
    - One of the simplest is to have the agent choose a random action at time step t with probability 1/t and to follow the greedy policy otherwise
    - A better approach would be to give some weight to actions that the agent hasn’t tried very often, while tending to avoid actions that are believed to be of low utility

# Exploration Function

- We can alter the $U(s)$ function to assign a higher utility to relatively unexplored state-action pairs
    - This amounts to an optimistic prior over the possible environments and causes the agent to behave as if there were wonderful rewards scattered all over the place

    $$
    U^+(s) \leftarrow \max_a f \Bigg ( \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma U^+(s)], N(s,a) \Bigg )
    $$

    - Where:
        - $U^+(s)$ represents the optimistic estimate of the utility (I.e. expected reward to go) of the state *s*
        - $N(s,a)$ represents the number of times an action *a* has been tried in state *s*
- ***The function $f(u,n)$ determines how greed (preference for high values of utility $u$) is traded off against curiosity (preference for actions that have not been tried often and have a low count $n$)***
    - The function should be increasing in u and decreasing n
    - There are many possible functions that can fit these conditions:
        - $f(u,n) = \begin{cases}
           R^+ &\text{if } n < N_e \\
           u & \text{otherwise}
        \end{cases}$
            - Where $N_e$ is a fixed parameter
            - This will have the effect of making the agent try each-state action pair atleast $N_e$  times
            - We use $U^+$ here because as exploration proceeds, the states and actions near the
            start state might well be tried a large number of times
                - If we used $U$, the more pessimistic utility estimate, then the agent would soon become disinclined to explore further afield
                - The use of $U^+$means that the benefits of exploration are propagated back from the edges of unexplored regions, so that actions that lead toward unexplored regions are weighted more
                highly, rather than just actions that are themselves unfamiliar

# Safe Exploration

- So far we have assumed that an agent is free to explore as it wishes—that any negative rewards serve only to improve its model of the world
    - That is, if we play a game of chess and lose, we suffer no damage (except perhaps to our pride), and whatever we learned will make us a better player in the next game
- We can end up in a bad state either because our model is unknown, and we actively choose to explore in a direction that turns out to be bad, or because our model is incorrect and we don’t know that a given action can have a disastrous result
    - Rather than using maximum-likelihood estimation to learn the transition model and choosing a policy based on this estimated model, a better idea is to choose a policy that works reasonably well for the whole range of models that have a reasonable chance of being the true model

## Bayesian RL

- Bayesian RL assumes a prior probability $P(h)$ over hypotheses $h$ about what the true model is; the posterior probability $P(h|e)$ is obtained in the usual way by Bayes’ rule given the observations to date
    - Then, if the agent has decided to stop learning, the optimal policy is the one that gives the highest expected utility

    $\pi^* = \argmax_a \sum_h P(h|e)U^\pi _h$

    - Where:
        - $U^\pi _h$ represents the expected utility, averaged over all possible start states, obtained by executing policy $\pi$ in model $h$

## Robust Control Theory

- Allows for a set of possible models H without assigning probabilities to them, and defines an optimal robust policy as one that gives the best outcome in the worst case over H

    $\pi^* = \argmax_{\pi} \min_h U^\pi _h$


# $\epsilon$-Greedy Stratgies

- Epsilon Decay: [https://stackoverflow.com/questions/53198503/epsilon-and-learning-rate-decay-in-epsilon-greedy-q-learning](https://stackoverflow.com/questions/53198503/epsilon-and-learning-rate-decay-in-epsilon-greedy-q-learning)
    - [https://medium.com/analytics-vidhya/stretched-exponential-decay-function-for-epsilon-greedy-algorithm-98da6224c22f](https://medium.com/analytics-vidhya/stretched-exponential-decay-function-for-epsilon-greedy-algorithm-98da6224c22f)
    - [https://aakash94.github.io/Reward-Based-Epsilon-Decay/](https://aakash94.github.io/Reward-Based-Epsilon-Decay/)
    - [https://stackoverflow.com/questions/53198503/epsilon-and-learning-rate-decay-in-epsilon-greedy-q-learning](https://stackoverflow.com/questions/53198503/epsilon-and-learning-rate-decay-in-epsilon-greedy-q-learning)