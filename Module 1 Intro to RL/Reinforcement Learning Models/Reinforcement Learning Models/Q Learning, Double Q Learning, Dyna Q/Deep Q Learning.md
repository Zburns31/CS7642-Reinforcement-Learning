# What is it?

- Deep Q Learning uses the Q-learning idea and takes it one step further. Instead of using a Q-table, **we use a Neural Network**
that takes a state and approximates the Q-values for each action based on that state
- We do this because using a classic Q-table is not very scalable. It might work for simple games, but in a more complex game with dozens of possible actions and game states the Q-table will soon get complex and cannot be solved efficiently anymore
- So now we **use a Deep Neural Network that gets the state as input, and produces different Q values for each action**
    - Then again we choose the action with the highest Q-value. The learning process is still the same with the iterative update approach, Â but instead of updating the Q-Table, here we update the weights in the neural network so that the outputs are improved

    ![Untitled](./Deep%20Q%20Learning/Untitled.png)


# Experience Replay

- One thing that can lead to our agent misunderstanding the environment is consecutive interdependent states that are very similar
- For example, if we're teaching a self-driving car how to drive, and the first part of the road is just a straight line, the agent might not learn how to deal with any curves in the road
    - From our self-driving car example, what happens with experience replay is that the initial experiences of driving in a straight line don't get put through the neural network right away
    - Instead, these experiences are saved into memory by the agent
    - Once the agent reaches a certain threshold *then* we tell the agent to learn from it
- ***So the agent is now learning from a batch of experiences. From these experiences, the agent randomly selects a uniformly distributed sample from this batch and learns from that***
    - Each experience is characterized by that state it was in, the action it took, the state it ended up in, and the reward it received
    - By randomly sampling from the experiences, this breaks the bias that may have come from the sequential nature of a particular environment, for example driving in a straight line

# How does it work?

- While **regular** Q-learning maps each state-action pair to its corresponding value, **deep** Q-learning uses a neural network to map input states to pairs via a three-step process:
    1. Initializing Target and Main neural networks
    2. Choosing an action
    3. Updating network weights using the Bellman Equation

## Main and Target Networks

- While these networks have the same overarching architectures, they have different weights. Every *N* steps, the weights from the Main network are copied to the Target network. Using both networks helps to stabilize the learning process so that the algorithm can learn more effectively

# Hyperparameter Tuning

- [https://pierpaolo28.github.io/blog/blog56/](https://pierpaolo28.github.io/blog/blog56/)

# Resources

- [https://huggingface.co/deep-rl-course/unit3/deep-q-algorithm?fw=pt](https://huggingface.co/deep-rl-course/unit3/deep-q-algorithm?fw=pt)
- [https://www.mlq.ai/deep-reinforcement-learning-q-learning/](https://www.mlq.ai/deep-reinforcement-learning-q-learning/)
- [https://builtin.com/artificial-intelligence/deep-q-learning](https://builtin.com/artificial-intelligence/deep-q-learning)
- [https://www.qwak.com/post/a-brief-introduction-to-reinforcement-learning-deep-q-learning](https://www.qwak.com/post/a-brief-introduction-to-reinforcement-learning-deep-q-learning)
- [https://medium.com/@qempsil0914/zero-to-one-deep-q-learning-part1-basic-introduction-and-implementation-bb7602b55a2c](https://medium.com/@qempsil0914/zero-to-one-deep-q-learning-part1-basic-introduction-and-implementation-bb7602b55a2c)
- [https://medium.com/@qempsil0914/deep-q-learning-part2-double-deep-q-network-double-dqn-b8fc9212bbb2](https://medium.com/@qempsil0914/deep-q-learning-part2-double-deep-q-network-double-dqn-b8fc9212bbb2)
- [https://rubikscode.net/2021/07/20/introduction-to-double-q-learning/](https://rubikscode.net/2021/07/20/introduction-to-double-q-learning/)
- [https://danieltakeshi.github.io/2019/07/14/per/](https://danieltakeshi.github.io/2019/07/14/per/)