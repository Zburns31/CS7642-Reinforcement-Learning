## What is it?

- Dyna-Q is a conceptual algorithm that illustrates how **real** and **simulated** experience can be combined in building a policy
- In Dyna-Q, learning and planning are accomplished by exactly the same algorithm, operating on real experience for learning and on simulated experience for planning
- **Planning** in RL terminology refers to using **simulated** experience generated by a **model** to find or improve a **policy** for interacting with a modeled environment ( *model-based*)

## How does it work?

Dyna is a blend of model-free and model-based methods.

1. First consider plain old Q learning, initialize our Q table and then we begin iterating, observe `s`, take action `a`, and then observe our new state `s'` and `reward r'`, and then update Q table with this experience tuple and repeat
2. When we augment Q learning with Dyna-Q, we had three new components, the first is that we add some “logic that enables us to learn models of T and R”, then we hallucinate an experience
    - Hallucinate these experiences, update our queue table and repeat this many times, maybe hundreds of times
    - This operation is very cheap compared to interacting with the real world
    - After we’ve iterated enough times down here maybe 100 maybe 200 times then we return and resume our interaction with the real world

    ![Untitled](./Dyna%20Q/Untitled.png)


### Q Learning Process:

1. Init Q Table
2. Observes S
3. Execute a, observe s
4. Update Q table with <s, a, s’, a’>

### Dyna Q Learning Process:

1. Init Q Table
2. Observes S
3. Execute a, observe s
4. Update Q table with <s, a, s’, a’>
5. Learn the transition and reward functions
    1. T’[s,a,s’] = ?
    2. R’[s,a] = ?
    3. How?
        1. s = random
        2. a = random
        3. s’ = infer from T table
        4. r = R[s,a] (R Table)
6. Simulate Experience
7. Update Q with <s, a, s’, a’>

## Dyna Q Algorithm

![Untitled](./Dyna%20Q/Untitled%201.png)

Algorithm Notes:

- **Step D:** Direct RL
- **Step E:** Model-Learning
- **Step F:** $Model(S,A)$ denotes the contents of the predicted next state and reward for the state-action pair $(s,a)$
    - Model Planning

```python
Initialize 𝑄(𝑠,𝑎) and 𝑀𝑜𝑑𝑒𝑙(𝑠,𝑎) for all 𝑠∈𝑆 and 𝑎∈𝐴.

Loop forever:

  (a) 𝑆 ← current (nonterminal) state
	(b) 𝐴 ← 𝜖-greedy(𝑆,𝑄)
	(c) Take action 𝐴; observe resultant reward, 𝑅, and state, 𝑆′
	(d) 𝑄(𝑆,𝐴) ← 𝑄(𝑆,𝐴)+𝛼[𝑅+𝛾max𝑎𝑄(𝑆′,𝑎)−𝑄(𝑆,𝐴)]
	(e) 𝑀𝑜𝑑𝑒𝑙(𝑆,𝐴) ← 𝑅,𝑆′ (assuming deterministic environment)
	(f) Loop repeat 𝑘 times:

    𝑆 ← random previously observed state
		𝐴 ← random action previously taken in 𝑆
		𝑅,𝑆′ ← 𝑀𝑜𝑑𝑒𝑙(Dyna 𝑆,Dyna 𝐴)
		𝑄(𝑆,𝐴) = 𝑄(𝑆,𝐴) + 𝛼[𝑅 + 𝛾max𝑎 𝑄(𝑆′,𝑎) − 𝑄(𝑆,𝐴)] # Note the notational differences
		Q(S, A) = (1 - 𝛼) * Q(Dyna S,Dyna A) + 𝛼 * (R + 𝛾 max𝑎 𝑄(Dyna 𝑆′,Dyna A')) # equivalent to the above formula
```

## Learning & Evaluating the Transition Function

$$
T[s,a,s'] = probability \space s,a \rightarrow s'
$$

### Dyna Q Process:

1. Init $T_c[]$ = 0.00001
2. While executing, observe s,a,s’
3. Increment $T \rightarrow T_c[s,a,s']$ += 1
4. Find $T /T_c$ transition probabilities

### Notes:

- T count (TC): when we have experience with the real world, we’ll get back on `<s,a,s’>` and we’ll just count how many times did it happen
- ***Initialize all of our T count values to be a very, very small number to avoid number divided by zero***
- Each time we interact with the real world in Q-learning, we observe `<s,a,r,s’>`, then we just increment that location in our T-count matrix

## Evaluation:

$$
T[s,a,s'] = \dfrac {T_c[s,a,s']} {\sum_i ^n T_c[s,a,s']}
$$

## Learning the Reward Function

$$
R'[s,a] = (1-\alpha)*R[s,a] \space + \space \alpha * r
$$

Where:

- R[s,a] represents the expected reward from taking action *a* in state *s*
- *r* represents the immediate reward

## Update Rule

Each time we update our model of the environment with a real world experience, we then simulate N times through random `<state, action>` pairs. To update the Q table with the information, we use the below formula:

$$
Q(s,a) = (1-\alpha) * Q(s,a) + \alpha * (R + \gamma \max_\alpha Q(s', a) - Q(S,A))
$$

# Dyna-Q+

- This agent keeps track for each state–action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment
    - The more time that has elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model of it is incorrect
        - To encourage behaviour that tests long-untried actions, a special “bonus reward” is given on simulated experiences involving these actions
- In particular, if the modelled reward for a transition is r, and the transition has not been tried in $\tau$ time steps, then planning updates are done as if that transition produced a reward of $\tau + k\sqrt{\tau}$, for some small $k$

# Resources:

- [https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb](https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb)
- [https://www.cs.mcgill.ca/~dprecup/courses/RL/Lectures/9-planning-2019.pdf](https://www.cs.mcgill.ca/~dprecup/courses/RL/Lectures/9-planning-2019.pdf)
- [https://notesonai.com/Dyna-Q+-+Planning+and+Learning](https://notesonai.com/Dyna-Q+-+Planning+and+Learning)
- [https://arxiv.org/pdf/cs/9605103.pdf](https://arxiv.org/pdf/cs/9605103.pdf)
- [https://arxiv.org/pdf/cs/9605103.pdf](https://arxiv.org/pdf/cs/9605103.pdf)