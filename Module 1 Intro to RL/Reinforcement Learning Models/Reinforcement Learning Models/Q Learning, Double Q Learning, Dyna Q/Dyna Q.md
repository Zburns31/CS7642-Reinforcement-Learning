## What is it?

- Dyna-Q is a conceptual algorithm that illustrates how **real** and **simulated** experience can be combined in building a policy
- In Dyna-Q, learning and planning are accomplished by exactly the same algorithm, operating on real experience for learning and on simulated experience for planning
- **Planning** in RL terminology refers to using **simulated** experience generated by a **model** to find or improve a **policy** for interacting with a modeled environment ( *model-based*)

## How does it work?

Dyna is a blend of model-free and model-based methods.

1. First consider plain old Q learning, initialize our Q table and then we begin iterating, observe `s`, take action `a`, and then observe our new state `s'` and `reward r'`, and then update Q table with this experience tuple and repeat
2. When we augment Q learning with Dyna-Q, we had three new components, the first is that we add some â€œlogic that enables us to learn models of T and Râ€, then we hallucinate an experience
    - Hallucinate these experiences, update our queue table and repeat this many times, maybe hundreds of times
    - This operation is very cheap compared to interacting with the real world
    - After weâ€™ve iterated enough times down here maybe 100 maybe 200 times then we return and resume our interaction with the real world

    ![Untitled](./Dyna%20Q/Untitled.png)


### Q Learning Process:

1. Init Q Table
2. Observes S
3. Execute a, observe s
4. Update Q table with <s, a, sâ€™, aâ€™>

### Dyna Q Learning Process:

1. Init Q Table
2. Observes S
3. Execute a, observe s
4. Update Q table with <s, a, sâ€™, aâ€™>
5. Learn the transition and reward functions
    1. Tâ€™[s,a,sâ€™] = ?
    2. Râ€™[s,a] = ?
    3. How?
        1. s = random
        2. a = random
        3. sâ€™ = infer from T table
        4. r = R[s,a] (R Table)
6. Simulate Experience
7. Update Q with <s, a, sâ€™, aâ€™>

## Dyna Q Algorithm

![Untitled](./Dyna%20Q/Untitled%201.png)

Algorithm Notes:

- **Step D:** Direct RL
- **Step E:** Model-Learning
- **Step F:** $Model(S,A)$ denotes the contents of the predicted next state and reward for the state-action pair $(s,a)$
    - Model Planning

```python
Initialize ğ‘„(ğ‘ ,ğ‘) and ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™(ğ‘ ,ğ‘) for all ğ‘ âˆˆğ‘† and ğ‘âˆˆğ´.

Loop forever:

  (a) ğ‘† â† current (nonterminal) state
	(b) ğ´ â† ğœ–-greedy(ğ‘†,ğ‘„)
	(c) Take action ğ´; observe resultant reward, ğ‘…, and state, ğ‘†â€²
	(d) ğ‘„(ğ‘†,ğ´) â† ğ‘„(ğ‘†,ğ´)+ğ›¼[ğ‘…+ğ›¾maxğ‘ğ‘„(ğ‘†â€²,ğ‘)âˆ’ğ‘„(ğ‘†,ğ´)]
	(e) ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™(ğ‘†,ğ´) â† ğ‘…,ğ‘†â€² (assuming deterministic environment)
	(f) Loop repeat ğ‘˜ times:

    ğ‘† â† random previously observed state
		ğ´ â† random action previously taken in ğ‘†
		ğ‘…,ğ‘†â€² â† ğ‘€ğ‘œğ‘‘ğ‘’ğ‘™(Dyna ğ‘†,Dyna ğ´)
		ğ‘„(ğ‘†,ğ´) = ğ‘„(ğ‘†,ğ´) + ğ›¼[ğ‘… + ğ›¾maxğ‘ ğ‘„(ğ‘†â€²,ğ‘) âˆ’ ğ‘„(ğ‘†,ğ´)] # Note the notational differences
		Q(S, A) = (1 - ğ›¼) * Q(Dyna S,Dyna A) + ğ›¼ * (R + ğ›¾ maxğ‘ ğ‘„(Dyna ğ‘†â€²,Dyna A')) # equivalent to the above formula
```

## Learning & Evaluating the Transition Function

$$
T[s,a,s'] = probability \space s,a \rightarrow s'
$$

### Dyna Q Process:

1. Init $T_c[]$ = 0.00001
2. While executing, observe s,a,sâ€™
3. Increment $T \rightarrow T_c[s,a,s']$ += 1
4. Find $T /T_c$ transition probabilities

### Notes:

- T count (TC): when we have experience with the real world, weâ€™ll get back on `<s,a,sâ€™>` and weâ€™ll just count how many times did it happen
- ***Initialize all of our T count values to be a very, very small number to avoid number divided by zero***
- Each time we interact with the real world in Q-learning, we observe `<s,a,r,sâ€™>`, then we just increment that location in our T-count matrix

## Evaluation:

$$
T[s,a,s'] = \dfrac {T_c[s,a,s']} {\sum_i ^n T_c[s,a,s']}
$$

## Learning the Reward Function

$$
R'[s,a] = (1-\alpha)*R[s,a] \space + \space \alpha * r
$$

Where:

- R[s,a] represents the expected reward from taking action *a* in state *s*
- *r* represents the immediate reward

## Update Rule

Each time we update our model of the environment with a real world experience, we then simulate N times through random `<state, action>` pairs. To update the Q table with the information, we use the below formula:

$$
Q(s,a) = (1-\alpha) * Q(s,a) + \alpha * (R + \gamma \max_\alpha Q(s', a) - Q(S,A))
$$

# Dyna-Q+

- This agent keeps track for each stateâ€“action pair of how many time steps have elapsed since the pair was last tried in a real interaction with the environment
    - The more time that has elapsed, the greater (we might presume) the chance that the dynamics of this pair has changed and that the model of it is incorrect
        - To encourage behaviour that tests long-untried actions, a special â€œbonus rewardâ€ is given on simulated experiences involving these actions
- In particular, if the modelled reward for a transition is r, and the transition has not been tried in $\tau$ time steps, then planning updates are done as if that transition produced a reward of $\tau + k\sqrt{\tau}$, for some small $k$

# Resources:

- [https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb](https://towardsdatascience.com/reinforcement-learning-model-based-planning-methods-extension-572dfee4cceb)
- [https://www.cs.mcgill.ca/~dprecup/courses/RL/Lectures/9-planning-2019.pdf](https://www.cs.mcgill.ca/~dprecup/courses/RL/Lectures/9-planning-2019.pdf)
- [https://notesonai.com/Dyna-Q+-+Planning+and+Learning](https://notesonai.com/Dyna-Q+-+Planning+and+Learning)
- [https://arxiv.org/pdf/cs/9605103.pdf](https://arxiv.org/pdf/cs/9605103.pdf)
- [https://arxiv.org/pdf/cs/9605103.pdf](https://arxiv.org/pdf/cs/9605103.pdf)