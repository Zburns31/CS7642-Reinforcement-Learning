# What is it?

- The A3C algorithm is a type of actor-critic algorithm, which means that it uses both a policy network (actor) and a value function network (critic) to learn to take actions in an environment
    - The policy network learns to select actions based on the observed state of the environment, while the value function network estimates the expected return of a given state-action pair
- The ***"asynchronous"*** aspect of the A3C algorithm refers to the fact that it trains multiple agents in parallel, each with their own copy of the environment, and updates the global network parameters asynchronously
    - This allows for more efficient use of computational resources and faster convergence
- The ***"advantage"*** aspect of the A3C algorithm refers to the use of an advantage function to estimate the relative value of each action taken by the policy network compared to the average value of all actions
    - This helps to reduce the variance in the gradient estimates and improves the stability of the learning process

# What is different than other Actor-Critic Methods?

- To reduce bias, it uses *n*-step returns with bootstrapping to learn the policy and value function
- It also uses concurrent actors to generate a broad set of experience samples in parallel

# How does it work?

**Actor Workers**

- One of the main sources of variance in DRL algorithms is how correlated and non-stationary online samples are
    - In value-based methods, we use a replay buffer to uniformly sample mini-batches of, for the most part, independent and identically distributed data
    - Unfortunately, using this experience-replay scheme for reducing variance is limited to off-policy methods, because on-policy agents cannot reuse data generated by previous policies
        - In other words, every optimization step requires a fresh batch of on-policy experiences
- Instead of using a replay buffer, what we can do in on-policy methods is have multiple workers generating experience in parallel and asynchronously updating the policy and value function
    - Having multiple workers generating experience on multiple instances of the environment in parallel decorrelates the data used for training and reduces the variance of the algorithm

    ![Untitled](./A3C%20Asynchronous%20Advantage%20Actor-Critic/Untitled.png)


**Using N-Step Estimates**

- N-step returns are used to improve the estimate of the expected return (or reward) of a given state-action pair
    - The expected return is a crucial component of the value function network, which is used to guide the policy network's actions in the environment
    - A traditional Monte Carlo approach for estimating the expected return involves waiting until the end of an episode and calculating the sum of the rewards from that point onward
    - However, in practice, it can take a large number of steps to reach the end of an episode, and this can lead to slow convergence and poor sample efficiency
- In the A3C algorithm, the use of N-step returns allows for more efficient updates to the value function network
    - Specifically, instead of waiting until the end of an episode to update the value function network, the agent can update the network after N steps using the N-step return estimate
        - This helps to reduce the variance of the gradient estimates and improves the stability of the learning process
- **N-Step Bootstrapping Estimates:**
    - Previously, we would use full returns for our advantage estimates: $A(S_t, A_t; \phi) = G_t - V(S_t; \phi)$
    - Now, we use N-Step returns with bootstrapping:

$$
A(S_t, A_t; \phi) = R_t + \gamma R_{t+1} + ... + \gamma^n R_{t+n} + \gamma^{n+1} V(S_{t+n+1}; \phi) - V(S_t; \phi)
$$

- Combining this with our A3C (policy gradient) method, we can use this to update the action probabilities:

$$
L_\pi(\theta) = - \dfrac{1} {N} \sum_{n=0}^N \Big [ A(S_t, A_t; \phi) \space \log \pi (A_t|S_t; \theta) + \beta H(\pi(S_t;\theta)) \Big ]
$$

- We also use the n-step return to improve the value function estimate. Notice the bootstrapping here. This is what makes the algorithm an actor-critic method

$$
L_v(\phi) = \dfrac{1} {N} \sum_{n=0}^N \Big [ \Big(R_t + \gamma R_{t+1} + ...+ \gamma^n R_{t+n} + \gamma^{n+1} V(S_{t+n+1}; \phi) - V(S_t;\phi) \Big )^2  \Big ]
$$

## Non-Blocking Model Updates

- One of the most critical aspects of A3C is that its network updates are asynchronous and lock-step free
    - Having a shared model creates a tendency for competent software engineers to want a blocking mechanism to prevent workers from overwriting other updates



## Generalized Advantage Estimation (GAE)

- Empirically, the Î»-return has been shown to produce better performance than simply using an n-step return
- [Advantage Functions & GAE](../Advantage%20Functions%20&%20GAE.md)

# Resources

- [https://paperswithcode.com/method/a3c](https://paperswithcode.com/method/a3c)
- [https://medium.com/sciforce/reinforcement-learning-and-asynchronous-actor-critic-agent-a3c-algorithm-explained-f0f3146a14ab](https://medium.com/sciforce/reinforcement-learning-and-asynchronous-actor-critic-agent-a3c-algorithm-explained-f0f3146a14ab)